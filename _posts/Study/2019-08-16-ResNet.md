---
layout: post
title: "ResNet(2015)"
date: 2019-08-13 00:00:00
excerpt: "ResNet 설명 및 Tensorflow  "  
tags:
- Study
categories:
- 공부
---
## Table of Contents 
1. [Introduction](#intro)
2. [Deep Residual Learning](#res)
3. [Network Architecture](#network)


### 1. Introduction<a name="intro"></a>
2012년의 Alexnet 이후로 Image Classification 분야의 대세는 딥러닝이 되었습니다. 현재 State-of-the-art는 모두 VGGNet, GoogleNet같은 "Very Deep"한 모델들입니다.   이러한 모델들의 특징은 모두 Convolution을 여러개 쌓아 "Depth" 가 깊은 모델들이라 할 수 있습니다. 그럼 여기서 한 가지 의문을 제기할 수 있습니다.  
바로 *Depth가 커지면 무조건 좋을까?*   
당연히 답은 아닙니다. 다음에서 보듯이 깊게 쌓은 Layer가 마냥 좋은 performance를 보여주진 않습니다.
![Graph1](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/1-res.PNG)  
  
그림에서 보다시피 56-layer가 20-layer에 비해 에러가 상대적으로 높은 것을 볼 수 있습니다. 나머지 36개의 Layer를 identity 로 구성해도 20-layer와 에러가 비슷해야 하는데, 그렇지 않다는 것입니다.  
이런 현상이 발생하는 원인은 바로 모델이 Deep해질수록 gradient vanishing/exploding 문제가 발생하게 되는데, 이런 문제를 바로 **degradation**이라 합니다.  
ResNet은 **degradation**문제를 "Residual Block"이라 부르는 shortcut-connection을 이용해 해결 하였습니다.  
  
#### Residual Mapping
![Block](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/2-res.PNG)  
$$ H(x) $$를 레이어의 출력이라 하면, $$ H(x) $$ 와 $$ x $$의 차이인 $$F(x) = H(x) - x $$ 를 학습하는 것입니다. 논문에서는 $$ H(x) $$를 직접 학습하는 것 보다 $$ F(x) + x $$를 학습하는게 쉬울것이라고 가정합니다. (We **hypothesize** that it is easier to optimize the residual mapping than to optimize the original)  
  
이러한 구조를 사용한 ResNet은 , 2015 ILSVRC에서 COCO, Imagenet 상관없이 모두 1st places를 차지했습니다. 이를 통해 residual learning은 특정한 모델에만 적용되는게 아니고 다른 분야에도 모두 적용할 수 있을 것이라는 예측을 할 수 있습니다.  
  
  
### 2. Deep Residual Learning <a name="res"></a>  
introduction에서 설명했듯이, 추가된 Layer들이 identity-mapping 이라면 Deep한 모델(identity가 추가된 모델)이 기존의 shallow 모델보다 에러가 낮아야하는데 그렇지 못합니다.   
이러한 이유는 바로 $$ H(x) $$라는 Nonlinear Layer가 학습이 잘 안된다는 것입니다. 따라서 우리는 Residual fuctnion $$ F(x) + x$$를 학습하게 됩니다.  
실제로, $$H(x)$$가 identity-mapping 이 Optimal인 레이어라면 학습 진행 과정에서 $$F(x) $$는 0이 될 것입니다. 하지만 real cases 들에서는 identity가 Optimal 한 경우는 없지만, reformulation 과정이 학습에 도움이 됩니다.  
  
#### Identity Mapping by Shortcuts
Residual Block을 다음과 같이 정의할 수 있습니다.  
$$ y = F(x,{W_{i}})+x $$  
만약에 F가 두개의 layer로 이루어 졌다면,  
$$ y = W_{2}\upsigma(W_{1}x) $$로 나타낼 수 있습니다.  또한 연산 $$ F + x $$ 는 element-wise 연산으로, dimension이 같아야 진행할 수 있습니다.  
만약 F와 x의 dimension이 다르다면, Linear Projection $$ W_{s} $$ 을 이용하여 다음과 같이 나타낼 수 있습니다.  
$$ y = F(x,{W_{i}})+W_{s}x $$  
여기서 F는 위의 예시처럼 2-Layer가 될 수도 있고, 여러 Layer를 나타낼 수도 있지만, 만약에 1-Layer라면 $$ y = W_{1}x + x $$ 로 Linear한 형태가 되어 의미가 없어집니다.  

  
### 3. Network Architecture<a name="network"></a> 
논문에서는 다양한 plain/residual 네트워크를 이용하여 실험을 진행하였습니다.  
![Network](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/3-res.PNG)  
#### 1) Plain Network
3 by 3 필터를 사용한 VGGnet의 구조와 유사하지만, 계산량을 줄이기 위해서 3개의 FC Layer를 사용한 VGGNet과 달리 global average pooling과 1개의 1000-way FC Layer를 사용하였습니다. 이를 통해 VGG-19에 비해 18%라는 계산량으로 모델을 구성할 수 있게 되었습니다.    
#### 2) Residual Network
1의 Plain Network를 기본으로, 중간에 shortcut connection을 삽입하였습니다. 위에서 dotted-line으로 된 shortcut의 경우는 Linear Projection을 이용해 dimension을 맞춰준 부분입니다.

