---
layout: post
title: "ResNet(2015)"
date: 2019-08-13 00:00:00
excerpt: "ResNet 설명 및 Tensorflow  "  
tags:
- Study
categories:
- 공부
---
## Table of Contents 
1. [Introduction](#intro)
2. [Deep Residual Learning](#res)


### 1. Introduction<a name="intro"></a>
2012년의 Alexnet 이후로 Image Classification 분야의 대세는 딥러닝이 되었습니다. 현재 State-of-the-art는 모두 VGGNet, GoogleNet같은 "Very Deep"한 모델들입니다.   이러한 모델들의 특징은 모두 Convolution을 여러개 쌓아 "Depth" 가 깊은 모델들이라 할 수 있습니다. 그럼 여기서 한 가지 의문을 제기할 수 있습니다.
바로 *Depth가 커지면 무조건 좋을까?*   
당연히 답은 아닙니다. 다음에서 보듯이 깊게 쌓은 Layer가 마냥 좋은 performance를 보여주진 않습니다.
![Graph1](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/1-res.PNG)  
  
그림에서 보다시피 56-layer가 20-layer에 비해 에러가 상대적으로 높은 것을 볼 수 있습니다. train error와 test error가 비슷하게 수렴하는 것을 보니 overfitting 문제는 아닙니다.  
이런 현상이 발생하는 원인은 바로 모델이 Deep해질수록 gradient vanishing/exploding 문제가 발생하게 되는데, 이런 문제를 바로 **degradation**이라 합니다.  
ResNet은 **degradation**문제를 "Residual Block"이라 부르는 shortcut-connection을 이용해 해결 하였습니다.  
  
#### Residual Mapping
![Block](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/2-res.PNG)  
$$ H(x) $$를 레이어의 출력이라 하면, $$ H(x) $$ 와 $$ x $$의 차이인 $$F(x) = H(x) - x $$ 를 학습하는 것입니다. 논문에서는 $$ H(x) $$를 직접 학습하는 것 보다 $$ F(x) + x $$를 학습하는게 쉬울것이라고 가정합니다. (We **hypothesize** that it is easier to optimize the residual mapping than to optimize the original)  
  
이러한 구조를 사용한 ResNet은 , 2015 ILSVRC에서 COCO, Imagenet 상관없이 모두 1st places를 차지했습니다. 이를 통해 residual learning은 특정한 모델에만 적용되는게 아니고 다른 분야에도 모두 적용할 수 있을 것이라는 예측을 할 수 있습니다.  
  
  
### 2. Deep Residual Learning <a name="res"></a>