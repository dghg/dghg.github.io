---
layout: post
title: "Recurrent Neural Network와 LSTM"
date: 2019-08-06 00:00:00
excerpt: "이 글은 CS231n 강의를 정리한 것입니다. "  
tags:
- ML
- DEEPLearning
categories:
- 공부
---
## Table of Contents
1. [Recurrent Neural Network](#rnn)
2. [Backpropagation](#back)
3. [LSTM](#lstm)

### 1. Recurrent Neural Network<a name="rnn"></a>
RNN 은 Neural Network의 한 종류로, 모델의 **flexibility**를 증가시키기 위해 고안되었습니다.  
일반적인 Neural Network의 경우, 입력과 출력이 고정(one-to-one) 되어있다는 단점이 있는데, 이를 해결하고자 RNN을 도입하였습니다.  
  
RNN은 **core cell**이라 부르는 노드를 중심으로 구성되어 있는데, 이 노드 덕분에 네트워크의 입력과 출력이 가변적인 길이를 가지게 될 수 있습니다. 이러한 특성을 가진 RNN은 문자 등을 처리하는 모델에 유용하게 쓰이고 있습니다.  
  
RNN 모델은 매 *time step*마다 hidden state인 를 갱신해나가며 입력 시퀀스를 처리해 나갑니다.  
여기서 hidden state는,  
$$ h_{t} = F_{w}(h_{t-1},x_{t}) $$
로, 이전 state와 입력의 함수로 구성됩니다.  

#### Vanila RNN
Activation 함수로 tanh함수를 이용하고,  출력인 y는 $$ h_{t} $$와 weight matrix인 W의 곱으로 구성됩니다.  

$$ h_{t} = tanh(W_{hh}h_{t-1}+W_{xh}x_{t}) $$  
$$ y_{t} = W_{hy}h_{t} $$
이고, weight matrix인 W는 모든 t에 대해 동일하게 적용됩니다.

#### Example : character-level-model
예제로는 다음 글자를 예측하는 character-level-model이 소개되었습니다. 예를 들어 $$ [h,e,l,l] $$를 입력으로 넣으면 출력은 다음글자들인 $$ [e,l,l,o] $$가 됩니다.   
우선 입력을 one-hot으로 인코딩하고, 모델의 입력으로 넣습니다.  
![RNN](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/9.PNG)  
이러면 output layer인 y 값이 만들어 지고, 정답인$$ [e,l,l,o] $$와 *Softmax Loss*를  이용해 parameter W를 갱신해 나갑니다.  
**Test Time**에서는 모든 문자를 넣는게 아니라, prefix만 입력으로 넣어주여야 합니다. 여기서는 첫 문자인 h가 prefix가 됩니다.  
그리고 softmax를 통과시킨 확률 분포에서 sampling 해 다음 입력으로 넣어 계속해서 진행시켜 나갑니다.
![RNN](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/10.PNG)  

### 2. Backpropagation<a name="back"></a>

### 3. LSTM(Long Short Term Memory)<a name="lstm"></a>
