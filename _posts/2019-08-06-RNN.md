---
layout: post
title: "Recurrent Neural Network와 LSTM"
date: 2019-08-06 00:00:00
excerpt: "이 글은 CS231n 강의를 정리한 것입니다. "  
tags:
- CS231N
- 딥러닝
- RNN
categories:
- 공부
---
## Table of Contents
1. [Recurrent Neural Network](#rnn)
2. [Backpropagation](#back)
3. [LSTM](#lstm)

### 1. Recurrent Neural Network<a name="rnn"></a>
RNN 은 Neural Network의 한 종류로, 모델의 **flexibility**를 증가시키기 위해 고안되었습니다.  
일반적인 Neural Network의 경우, 입력과 출력이 고정(one-to-one) 되어있다는 단점이 있는데, 이를 해결하고자 RNN을 도입하였습니다.  
  
RNN은 **core cell**이라 부르는 노드를 중심으로 구성되어 있는데, 이 노드 덕분에 네트워크의 입력과 출력이 가변적인 길이를 가지게 될 수 있습니다. 이러한 특성을 가진 RNN은 문자 등을 처리하는 모델에 유용하게 쓰이고 있습니다.  
  
RNN 모델은 매 *time step*마다 hidden state인 를 갱신해나가며 입력 시퀀스를 처리해 나갑니다.  
여기서 hidden state는,  
$$ h_{t} = F_{w}(h_{t-1},x_{t}) $$
로, 이전 state와 입력의 함수로 구성됩니다.  

#### Vanila RNN
Activation 함수로 tanh함수를 이용하고,  출력인 y는 $$ h_{t} $$와 weight matrix인 W의 곱으로 구성됩니다.  

$$ h_{t} = tanh(W_{hh}h_{t-1}+W_{xh}x_{t}) $$  
$$ y_{t} = W_{hy}h_{t} $$
이고, weight matrix인 W는 모든 t에 대해 동일하게 적용됩니다.

#### Example : character-level-model
예제로는 다음 글자를 예측하는 character-level-model이 소개되었습니다. 예를 들어 $$ [h,e,l,l] $$를 입력으로 넣으면 출력은 다음글자들인 $$ [e,l,l,o] $$가 됩니다.   
우선 입력을 one-hot으로 인코딩하고, 모델의 입력으로 넣습니다.  
![RNN](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/9.PNG)  
이러면 output layer인 y 값이 만들어 지고, 정답인$$ [e,l,l,o] $$와 *Softmax Loss*를  이용해 parameter W를 갱신해 나갑니다.  
**Test Time**에서는 모든 문자를 넣는게 아니라, prefix만 입력으로 넣어주여야 합니다. 여기서는 첫 문자인 h가 prefix가 됩니다.  
그리고 softmax를 통과시킨 확률 분포에서 sampling 해 다음 입력으로 넣어 계속해서 진행시켜 나갑니다.
![RNN](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/10.PNG)  
  
  
  
### 2. Backpropagation<a name="back"></a>
RNN에서 backpropagation는 **"Backpropagation Through Time"** 방식으로 진행하게 되는데, 초기상태 $$ h_{0} $$ 부터 현재 단계인 t단계 $$ h_{t} $$ 까지의 loss값을 합한 뒤 이를 이용해 backpropagation 합니다. 하지만 이러한 방식으로 진행해 나간다면, 시퀀스의 길이가 길어질 경우(t가 커질 경우) 학습속도가 느려진다는 단점이 있습니다.  
이에 대한 대안으로 현재 단계부터 일정 단계 이전까지만 backprop 하는 "**Truncated Backpropagation Through Time**"을 이용하게 됩니다. 
  
#### Gradient exploding/vanishing
Vanila RNN의 경우 $$ h_{t} $$를 계산하는데 weight matrix와 연결되어 있습니다. ( $$ h_{t} = tanh(W_{hh}h_{t-1}+W_{xh}x_{t}) $$  )  
이렇기 때문에 $$ h_{0} $$ 에 대한 gradient를 계산할 경우 반복적으로 W를 곱해나가야 되고, 이는 gradient exploding/vanishing의 문제가 발생하게 됩니다. 이때 exploding의 경우 gradient의 최댓값을 제한하는 *gradient clipping*을 사용하면 되지만, vanishing의 경우  는 h와 W가 연결되어있는 RNN 구조를 변경해야 합니다. 이를 위해 나온것이 다음에서 설명할 **LSTM** 입니다.  
  
  
### 3. LSTM(Long Short Term Memory)<a name="lstm"></a>
LSTM은 기존의 RNN에서 gradient 문제를 해결하여 gradient flow가 잘 되기 위해 고안되었습니다. 기본적으로 LSTM은 기존과 동일하게 hidden state $$ h_{t} $$와 여기에 추가적으로 cell state $$ c_{t} $$를 도입하였습니다.  
여기에다 weight matrix W의 크기를 늘려, state 계산에 필요한 여러 게이트들을 도입하였습니다. 여기서 W는 다음과 같습니다.

$$
    \begin{matrix}W_{xh_1} & W_{hh_1} \\ W_{xh_2} & W_{hh_2} \\ W_{xh_3} & W_{hh_3} \\ W_{xh_4} & W_{hh_4} \\ \end{matrix}
$$

  이를 이용하여 input gate, forget gate, output gate, gate gate(?)를 만들어 냅니다.  
첫번째로 **input gate**는 cell에 입력할지 결정하는 gate가 되고,  $$ i = \sigma(W_{xh_1}x_{t}+W_{hh_2}h_{t-1}) $$

두번째로 **forget gate**는 cell에 이전 cell을 얼마나 반영할지 결정하는 gate가 되고,  
$$ f = \sigma(W_{xh_2}x_{t}+W_{hh_2}h_{t-1}) $$  

세번째로 **output gate**는 다음 hidden state에 얼마나 $$ c_{t}$$를 반영할지 결정하는 게이트입니다.  
$$ o = \sigma(W_{xh_3}x_{t}+W_{hh_3}h_{t-1}) $$

마지막으로 **gate gate** 는 input과 마찬가지로 cell에 얼마나 입력할지 결정하는 gate이고, 
$$ g = tanh(W_{xh_3}x_{t}+W_{hh_3}h_{t-1}) $$

이 4개의 gate를 이용해 $$ c_{t} $$ 와 $$ h_{t} $$를 갱신해 나갑니다.  
먼저 $$ c_{t} $$ 의 경우 이전 셀인 $$ c_{t-1} $$와 $$ i, f, g $$를 이용해 $$ c_{t} = f⊙c_{t-1} + i⊙g $$로 나타낼 수 있습니다.  
또한 $$ h_{t} $$ 는 $$ h_{t} = o⊙tanh(c_{t}) $$로 계산할 수 있습니다.  

이렇듯 Vanila RNN과 달리 cell state를 도입함으로써,  $$c_{t}$$의 Backprop 과정에서 forget-gate와 $$c_{t-1}$$를 곱함으로써 계속 다른 $$f$$와의 곱셈으로 gradient 문제가 발생하는 것을 방지할 수 있습니다.  
또한, 계산 시 $$W$$ matrix의 영향을 받지 않으므로 gradient flow가 잘 될 수 있게 해줍니다.  

## 요약
1. RNN은 시퀀스 데이터를 처리하기 위해 고안되었다.  
2. 가변길이의 데이터를 처리하기 위해 hidden state를 도입하였다.  
3. hidden state 계산 과정에서 gradient 문제가 발생하여 LSTM을 도입하였다.  


  