---
layout: post
title: "Review AlexNet(2012)"
date: 2019-07-01 00:00:00
excerpt: Alexnet 정리와 Tensorflow로의 구현" 
tags:
- ML
- DEEPLearning
categories:
- 공부
---

# ImageNet Classification With Deep Convolutional Neural Network
Alex Krizhevsky  
University of Toronto  
kriz@cs.utoronto.ca


## 1. Introduction
현재의 대부분 Object 인식은 머신러닝 기법을 사용한다. 간단한 인식은 작은 datasets 으로도 좋은 결과(accuracy)를 얻을 수 있다. 예를 들어 현재 MNIST digit-recognition task의 가장 좋은 에러율은 <0.3%로 사람과 비슷하다 그러나 현실에서의 object 인식 과정은 많은 variablity가 존재하므로 더 큰 datasets으로 학습을 진행해야 한다. AlexNet의 경우 22,000개의 label을 가진 15 million 이상의 datasets인 ImageNet datasets을 사용하였다. 또한 AlexNet은 학습에서 두개의 GTX 580 3GB GPUs를 사용하여 진행하였다.

## 2. Datasets
ImageNet dataset을 사용하였다. Input을 일정하게 하고자 모든 이미지들을 256 X 256로 down-sampled 하고, 평균을 빼서 zero-centered 된 이미지를 사용하였다.

## 3. Architectures
### 3.1 ReLU Nonlinearlity
saturation을 피하고 빠른 수렴속도를 위해 Sigmoid나 tanh 대신 ReLU를 사용하였다.
### 3.2 Training on Multiple GPUs
### 3.3 Local Response Normalization
![LRN](https://github.com/dghg/dghg.github.io/raw/master/_posts/img/1.PNG)  
ReLU를 사용하면 Input을 normalization 하지 않아도 된다. 
그러나 generalization을 위해 ( ReLU 값이 한없이 커질수 있어서) LRN을 사용하였다. 이를 통해 주변 kernal에 비해 너무 큰 값 등을 보정한다. 
이 정규화 기법은 "adjacnet"한 n개의 kernal을 이용하여 정규화 하는 것이다. 여기서 k, n, α, β는 hyperparameter로 AlexNet에서는 k = 2, n = 5, α = 10^−4, β = 0.75을 사용하였다.  
이러한 방식을 통해 AlexNet은 top-1 와 top-5 에러를 각각 1.4%와 1.2% 낮출 수 있었다.


### 3.4 Overlapping Pooling
Stride와 필터의 크기가 같은 일반적인 Pooling Layer와 달리 s(stride) = 2 , z(size) = 3으로 하는 Overlapping Pooling을 사용하였다. 이를 통해 기존(s=z=2)보다 top-1 , top-5 error을 0.4%, 0.3% 감소시켰다.


## 4. Reducing Overfitting
계산량이 매우 적은 두가지의 Data Augmentation 기법들을 사용하였다. 이를 통해 1) CPU에서 Image 생성, 2) GPU에서 이전 batch training 을 동시에 진행하였다.
### 4.1 Data Augmentation
1. *Image 추출과 Reflection*  
원본 data로 부터 224\*224의 Random patches 추출 하고 horizontal reflection을 진행하였다. 이를 통해 기존의 2048배의 Data를 얻을 수 있다.  
test time에서는 5개의 patch(4개의 코너와 중앙)과 reflection으로 총 10개의 patch를 얻고, 10개의 예측값의 평균을 사용하였다.
2. *PCA on the RGB set*

### 4.2 Dropout
p=0.5로 첫번째와 두번째의 FC Layer에 dropout을 사용하였다.

## 5. Details of Learning / Implementation
### 5.1 Architecture
총 5개의 CONV Layer, 3개의 FC Layer를 사용하였다.
![ARCH](https://github.com/dghg/dghg.github.io/blob/master/_posts/img/2.PNG?raw=true)
#0 Input : 227\*227\*3의 Input  
#1 CONV : 96개의 11\*11 Filter 사용 (padding=0,stride=4)  
#1 CONV : Output 55\*55\*96 , Parameter = 34848  
#1 Pooling, LRN : s=2,z=3의 pooling 사용  
#1 Pooling, LRN : Output 27\*27\*96  
#2 CONV : 256개의 5\*5 Filter 사용 (padding=2, stride=1)  
#2 CONV : Output 27\*27\*256 , Parameter = 614400  
#2 Pooling, LRN : Output 13\*13\*256  
#3 CONV : 384개의 3\*3 Filter 사용 (padding=1, stride=1)  
#3 CONV : Output 13\*13\*384 , Parameter = 884736  
#4 CONV : 384개의 3\*3 Filter 사용 (padding=1, stride=1)  
#4 CONV : Output 13\*13\*384, Parameter = 1327104  
#5 CONV : 256개의 3\*3 Filter 사용 (padding=1, stride=1)  
#5 CONV : Output 13\*13\*256, Parameter = 884736  
#5 Pooling : Output 6\*6\*256  
#6 FC1 : 4096의 FC Layer  
#6 FC1 : Output 4096, Parameter = 37748736  
#7 FC2 : 4096의 FC Layer  
#7 FC2 : Output 4096, Parameter = 16777216  
#8 FC3 : 1000의 FC Layer  
#8 FC3 : Output 1000, Parameter = 4096000  

### 5.2 Details
Size가 128인 Batch, SGD, 0.9의 momentum, 0.0005의 weight decay를 사용하였다.  
또한 각 layer의 weight들을 N(0,0.01)의 분포로 초기화 하였다. bias의 경우 #1,3,4 CONV, FC는 1로, 나머지는 0으로 초기화하였다.  
그리고 모든 Layer의 learning rate는 0.01로 초기화하고, validation error가 줄어드는게 멈추면 10으로 나누어 사용하였다. 

### 5.3 Implementation
ImageNet 데이터셋이 너무 큰 관계로.. Cat and Dog 데이터셋(https://www.kaggle.com/tongpython/cat-and-dog/)을 사용하였다 . 또한 구현에 있어 Finetune AlexNet with Tensorflow(https://github.com/kratzert/finetune_alexnet_with_tensorflow)를 참고하였다. 또한 Google Colab을 이용하였다.


#### 1) Graph 생성
그래프를 생성하기 위한 Alexnet Class  
num_classes개의 score 값을 반환한다. 
~~~
class Alexnet(object):
    def __init__(self, x, dropout=0.5, num_classes=1000):
        self.X=x
        self.dropout=dropout
        self.num_classes=num_classes
        self.create_graph()
        
    def create_graph(self):
        conv_1 = self.conv2d(self.X, 3, 96, 11, 11, 4, padding='VALID', name='conv_1')
        conv_1 = tf.nn.local_response_normalization(conv_1, 2, 1e-04, 0.75, 1)
        pool_1 = tf.nn.max_pool(conv_1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool_1')
        
        conv_2 = self.conv2d(pool_1, 96, 256, 5, 5, 1, name='conv_2')
        conv_2 = tf.nn.local_response_normalization(conv_2, 2, 1e-04, 0.75, 1)
        pool_2 = tf.nn.max_pool(conv_2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='VALID', name='pool_2')
                
        conv_3 = self.conv2d(pool_2, 256, 384, 3, 3, 1, name='conv_3')
        conv_4 = self.conv2d(conv_3, 384, 384, 3, 3, 1, name='conv_4')
        conv_5 = self.conv2d(conv_4, 384, 256, 3, 3, 1, name='conv_5')
        
        pool_5 = tf.nn.max_pool(conv_5, ksize=[1,3,3,1], strides=[1, 2, 2, 1], padding='VALID', name='pool_5')
        
        pool_flatten = tf.reshape(pool_5, [-1, 6*6*256])
        
        fc_6 = self.fc(pool_flatten, 6*6*256, 4096, 'fc_6')
        dropout_6 = tf.nn.dropout(fc_6, keep_prob = self.dropout)
        
        fc_7 = self.fc(dropout_6, 4096, 4096, 'fc_7')
        dropout_7 = tf.nn.dropout(fc_7, keep_prob = self.dropout)
        
        self.score =  self.fc(dropout_7, 4096, self.num_classes, 'score')
    
    def conv2d(self, inp, input_channels, filter_num, filter_h, filter_w, strides, name, padding='SAME'):
        with tf.variable_scope(name) as scope:
            weights = tf.get_variable('weights', shape=[filter_h, filter_w, input_channels, filter_num])
            biases = tf.get_variable('biases', shape=[filter_num])
        conv = tf.nn.conv2d(inp, weights, strides=[1, strides, strides, 1], padding=padding, name=name)
        conv = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))
        print(conv.shape)
        return tf.nn.relu(conv)
    
    def fc(self, inp, input_channels, output_channels, name):
        with tf.variable_scope(name) as scope:
            weights = tf.get_variable('weights', shape=[input_channels, output_channels], initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.01))
            biases = tf.get_variable('biases', shape=[output_channels])
            fc = tf.nn.xw_plus_b(inp, weights, biases, name=scope.name)
        return tf.nn.relu(fc)

~~~

#### 2) Dataset 처리
Dataset을 불러와 batch 단위로 반환하기 위한 클래스이다.
먼저, init을 통해 각종 hyperparameter들을 초기화해준 후,  
read_data()함수와 shuffle_data()로 데이터를 읽는다.
~~~c
    def __init__(self, path, batch_size, num_classes):
        self.batch_size = batch_size
        self.num_classes = num_classes
        self.path = path # Image가 저장된 경로
        self.batch_pointer=0
        self.data_size=0
        self.read_data(path)      
        self.shuffle_data()
~~~

~~~
    def read_data(self, path): #data를 image,label로 저장
        data_list = os.listdir(path)
        self.data_size = len(data_list)
        
        self.images = []
        self.labels = []
        
        for i in range(self.data_size):
            items = data_list[i].split('.')[0]
            self.images.append(data_list[i])
            print("Data Loading.... Image : {}, Labels : {}".format(data_list[i],items))
            if(items=='cat'):
                self.labels.append(0)
            else:
                self.labels.append(1)
     
        self.convert_data()
~~~
데이터를 읽어와 images 배열에 파일이름을 저장하고, labels에 Y값을 저장한다.(cat = 0, dog = 1)
이후 convert_data()함수를 이용해 image파일을 numpy array로 변환한다.
~~~
    def convert_data(self): # Data들을 227*227*3 로 변환
        self.images_converted = np.ndarray([self.data_size,227,227,3])
        for i in range(self.data_size):
            tmp = os.path.join(self.path, self.images[i])
            img = cv2.imread(tmp)
            img = cv2.resize(img, (227,227))
            img = img.astype(np.float32)
            self.images_converted[i] = img
~~~

이제 next_batch()함수 호출을 통해 배치 단위로 데이터를 읽어들일 수 있다.

~~~
    def next_batch(self): # Batch 반환
        if(self.batch_pointer>=self.data_size):
            self.batch_pointer=0

        # Get batch
        X = self.images_converted[self.batch_pointer:self.batch_pointer+self.batch_size]
        Y = self.labels[self.batch_pointer:self.batch_pointer+self.batch_size]
        
        #Update pointer
        self.batch_pointer+=self.batch_size
        
        images = np.ndarray([np.shape(X)[0], 227, 227, 3])
        labels = np.zeros([np.shape(Y)[0], self.num_classes])
        
        for i in range(np.shape(X)[0]):
            images[i]=X[i]
            labels[i][self.labels[i]]=1
        
        return images, labels
~~~

